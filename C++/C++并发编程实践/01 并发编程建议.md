# 01 并发编程建议

我们尽可能地使用并发，唯一让我们放弃并发的原因是收益比不上成本。

而我们进行并发编程时，要依次注意线程安全，异常安全(exception-safety)，线程高效。

## 1 线程管理

本章将从基本开始：启动一个线程，等待这个线程结束，或放在后台运行。再看看怎么给已经启动的线程函数传递参数，以及怎么将一个线程的所有权从当前std::thread对象移交给另一个。最后，再来确定线程数，以及识别特殊线程。

### 启动线程

启动线程，实际上就是使用函数构造`std::thread`对象，此时**提供的函数对象会复制到新线程的存储空间当中**，函数对象的执行和调用都在线程的内存空间中进行。 

> 注意语法解析：
>
> 如果你传递了一个临时变量，而不是一个命名的变量；C++编译器会将其解析为函数声明，而不是类型对象的定义。
>
> ```cpp
> std::thread my_thread(background_task());
> ```
>
> 这里相当与声明了一个名为my_thread的函数，这个函数带有一个参数(函数指针指向没有参数并返回background_task对象的函数)，返回一个std::thread对象的函数，而非启动了一个线程。
>
> 为避免语法解析，可以使用`{}`
>
> ```cpp
> std::thread my_thread{background_task()};
> ```
>
> 

当构造了`std::thread`对象后，我们就需要考虑销毁的问题（即内存所有权在谁手里），而在销毁之前，我们对于线程有两种态度：

- 等待线程结果（`join`）
- 不等待，继续主程序（`detch`）

值得注意的是，`detch`会打破线程与对象的关系，如果线程由局部变量构造`detch`后可能因为局部变量的销毁造成悬空指针，因此**使用一个能访问局部变量的函数去创建线程是一个糟糕的主意**。

我们建议：

> 1. 除非十分确定线程会在函数完成前结束，否则不要使用一个能访问局部变量的函数去创建线程
> 2. 可以使用`join`确保线程在函数完成前结束
> 3. **将数据复制到线程中，而非复制到共享数据中**

### `join`

调用``join()``的行为，不仅是确保线程结束，还清理了线程相关的存储部分，这样`std::thread`对象将不再与已经完成的线程有任何关联。这意味着，只能对一个线程使用一次`join()`；一旦已经使用过`join()`，`std::thread`对象就不能再次加入了，当对其使用`joinable()`时，将返回否（*false*）。

如果想要分离一个线程，可以在线程启动后，直接使用`detach()`进行分离。如果打算等待对应线程，则**需要细心挑选调用join()的位置**。**当在线程运行之后产生异常，在join()调用之前抛出，就意味着很这次调用会被跳过。**

我们选择了`join`，但是却没有保障它的调用，因此**没有保证在发生异常时能确保访问本地状态的线程退出后，函数才结束，**这是很危险的，我们没有确保线程**异常安全(exception-safety)**。

我们提出两种解决方案：

> 1. `try/catch`语句
> 2. 创建RAII类型的类，将`join`写入析构函数

#### try/catch

清单 2.2 等待线程完成

```cpp
struct func; // 定义在清单2.1中
void f()
{
int some_local_state=0;
func my_func(some_local_state);
std::thread t(my_func);
try
{
 do_something_in_current_thread();
}
catch(...)
{
 t.join();  // 1
 throw;
}
t.join();  // 2
}
```

清单2.2中的代码使用了`try/catch`块确保访问本地状态的线程退出后，函数才结束。当函数正常退出时，会执行到②处；当函数执行过程中抛出异常，程序会执行到①处。`try/catch`块能轻易的捕获轻量级错误，所以这种情况，并非放之四海而皆准。

#### RAII

清单 2.3 使用RAII等待线程完成

```cpp
class thread_guard
{
std::thread& t;
public:
explicit thread_guard(std::thread& t_):
 t(t_)
{}
~thread_guard()
{
 if(t.joinable()) // 1
 {
   t.join();      // 2
 }
}
thread_guard(thread_guard const&)=delete;   // 3
thread_guard& operator=(thread_guard const&)=delete;
};

struct func; // 定义在清单2.1中

void f()
{
int some_local_state=0;
func my_func(some_local_state);
std::thread t(my_func);
thread_guard g(t);
do_something_in_current_thread();
}    // 4
```

当线程执行到④处时，局部对象就要被逆序销毁了。因此，thread_guard对象g是第一个被销毁的，这时线程在析构函数中被加入②到原始线程中。即使do_something_in_current_thread抛出一个异常，这个销毁依旧会发生。

注意，对于并发编程，拷贝和赋值是很危险的，可能会造成线程的丢失，也会导致锁的所有权滥用。

我们建议：

> 1. 尽量删除线程相关的类的拷贝构造函数和拷贝赋值操作

### ``detch``

分离线程为守护线程(*daemon threads*),UNIX中守护线程是指，且没有任何用户接口，并在后台运行的线程。这种线程的特点就是长时间运行。"发后即忘"(*fire and forget*)的任务就使用到线程的这种方式。

值得注意的是，当`detch`后，主线程不能与之产生直接交互，不再有`std::thread`对象能引用它。

> 《Effective C++》
>
> 当我使用术语“接口”时，我一般是指函数的签名（signature）或class的可访问元素（例如我可能会说class的“public接口”，“protected接口”或“private接口”），或是针对某template类型参数需为有效的一个表达式。

### 向线程函数传参

#### 引用与`std::ref`

[解析C++中std::ref]([(4条消息) 解析C++中std::ref_leapmotion的博客-CSDN博客_c++ std::ref](https://blog.csdn.net/leapmotion/article/details/120338292?ops_request_misc=%7B%22request%5Fid%22%3A%22167465907116782429743915%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=167465907116782429743915&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-120338292-null-null.142^v71^insert_chatgpt,201^v4^add_ask&utm_term=std%3A%3Aref&spm=1018.2226.3001.4187))

[C++11 的 std::ref 用法]([C++11 的 std::ref 用法 | 拾荒志 (murphypei.github.io)](https://murphypei.github.io/blog/2019/04/cpp-std-ref))

在**启动线程**里我们提到，使用函数构造`std::thread`对象时，**提供的函数对象会复制到新线程的存储空间当中**，函数对象的执行和调用都在线程的内存空间中进行。而传递参数时，也会把参数拷贝到线程的独立内存中。即使参数是引用的形式，也会先拷贝到线程内后，再进行引用，然后传递给线程中的函数。以`std::bind`为例，`std::bind`首先将传入的参数存放起来，等到要调用bind的函数时，就将参数传入，而这里没有保存传入参数的引用，只能保存一份参数的拷贝，如果使用我们上边说的“int& a = b”语法，_Binder类（编译器为`std::bind`构建的中间类）中无法保存b的引用，自然调用时传入的就不是b的引用。

而`std::ref`实际上是通过`reference_wrapper`将传入参数的地址保存，使用是通过地址取出来值进而调用函数。

我们建议：

> 1. `std::bind`使用的是参数的拷贝而不是引用，当可调用对象期待入参为引用时，必须显示利用`std::ref`来进行引用绑定。
>
> 2. 多线程`std::thread`的可调用对象期望入参为引用时，也必须显式通过`std::ref`来绑定引用进行传参

实际上，对于`std::thread`构造函数和`std::bind`操作，我们经常**传递一个成员函数指针作为线程函数，并提供一个合适的对象指针作为第一个参数**

```cpp
class X
{
public:
  void do_lengthy_work();
};
X my_x;
std::thread t(&X::do_lengthy_work,&my_x); // 1
```



#### 参数所有权与`std::move`

再次强调，内存所有权是很重要的，当我们需要在线程内完全掌握参数的所有权时，需要使用`std::move`。

```cpp
void process_big_object(std::unique_ptr<big_object>);

std::unique_ptr<big_object> p(new big_object);
p->prepare_data(42);
std::thread t(process_big_object,std::move(p));
```

在`std::thread`的构造函数中指定``std::move(p)``，big_object对象的所有权就被首先转移到新创建线程的的内部存储中，之后传递给process_big_object函数。

我们建议：

> 1. 对于
> 2. 当我们希望在线程中析构传入的参数对象时，使用`std::move`



### 线程所有权与`std::move`

**`std::thread`可移动(*movable*)，但不可拷贝(*cpoyable*)。**正如上面所说的，删除了拷贝构造函数和拷贝赋值操作，这也意味着，不能通过赋一个新值给`std::thread`对象的方式来"丢弃"一个线程。

```cpp
void some_function();
void some_other_function();
std::thread t1(some_function);            // 1
std::thread t2=std::move(t1);            // 2
t1=std::thread(some_other_function);    // 3
std::thread t3;                            // 4
t3=std::move(t2);                        // 5
t1=std::move(t3);                        // 6 赋值操作将使程序崩溃
```

`std::thread`支持移动有很多好处：

1. **意味着线程的所有权可以在函数外进行转移**，此时函数可以返回`std::thread`对象

   ```cpp
   std::thread f()
   {
   void some_function();
   return std::thread(some_function);
   }
   
   std::thread g()
   {
   void some_other_function(int);
   std::thread t(some_other_function,42);
   return t;
   }
   ```

2. **意味着允许`std::thread`实例可作为参数进行传递**

   ```cpp
   void f(std::thread t);
   void g()
   {
   void some_function();
   f(std::thread(some_function));
   std::thread t(some_function);
   f(std::move(t));
   }
   ```

3. **意味着可以创建线程管理类的实例，并且拥有其线程的所有权**

   ```cpp
   class scoped_thread
   {
   std::thread t;
   public:
   explicit scoped_thread(std::thread t_):                 // 1 新线程是直接传递到                                                           scoped_thread，而非                                                          创建一个独立的命名变量
    t(std::move(t_))
   {
    if(!t.joinable())                                     // 2
      throw std::logic_error(“No thread”);
   }
   ~scoped_thread()
   {
    t.join();                                            // 3
   }
   scoped_thread(scoped_thread const&)=delete;
   scoped_thread& operator=(scoped_thread const&)=delete;
   };
   
   struct func; // 定义在清单2.1中
   
   void f()
   {
   int some_local_state;
   scoped_thread t(std::thread(func(some_local_state)));    // 4
   do_something_in_current_thread();
   }                           
   ```

   注意，**当某个对象转移了线程的所有权后，它就不能对线程进行加入或分离。**

我们建议：

> 1. 设计线程管理类时，使用`std::move`移动线程所有权，并且删除拷贝构造函数和拷贝赋值操作

### 其他

#### 线程数量

`std::thread::hardware_concurrency()`，将返回能同时并发在一个程序中的线程数量。

#### 识别线程

线程标识类型是`std::thread::id`，可以通过两种方式进行检索。

- 第一种，可以通过调用`std::thread`对象的成员函数`get_id()`来直接获取。如果`std::thread`对象没有与任何执行线程相关联，`get_id()`将返回`std::thread::type`默认构造值，这个值表示“没有线程”。
- 第二种，当前线程中调用`std::this_thread::get_id()`(这个函数定义在`<thread>`头文件中)也可以获得线程标识。

`std::thread::id`对象可以自由的拷贝和对比,因为标识符就可以复用。如果两个对象的std::thread::id相等，那它们就是同一个线程，或者都“没有线程”。如果不等，那么就代表了两个不同线程，或者一个有线程，另一没有。



## 2 线程间共享数据

- 共享数据带来的问题
- 使用互斥量保护数据
- 数据保护的替代方案

### 条件竞争

条件竞争通常是时间敏感的，所以**程序以调试模式运行时，它们常会完全消失**，因为调试模式会影响程序的执行时间(即使影响不多)。

#### 使用互斥量保护共享数据

我们不推荐实践中直接去调用成员函数，因为调用成员函数就意味着，必须记住在每个函数出口都要去调用``unlock()``，也包括异常的情况。C++标准库为互斥量提供了一个RAII语法的模板类`std::lack_guard`，其会在构造的时候提供已锁的互斥量，并在析构的时候进行解锁，从而保证了一个已锁的互斥量总是会被正确的解锁。

```cpp
#include <list>
#include <mutex>
#include <algorithm>

std::list<int> some_list;    // 1
std::mutex some_mutex;    // 2

void add_to_list(int new_value)
{
std::lock_guard<std::mutex> guard(some_mutex);    // 3
some_list.push_back(new_value);
}

bool list_contains(int value_to_find)
{
std::lock_guard<std::mutex> guard(some_mutex);    // 4
return std::find(some_list.begin(),some_list.end(),value_to_find) != some_list.end();
}
```

`std::lack_guard`的作用仅仅是上锁，并让锁在作用域外释放，只是对`lock() unlock()`的化简。

我们建议：

> 1. 互斥量与被保护的数据放在一个类中
> 2. 不使用`unlock()`，而是使用`std::lock_guard + {}`的组合

当成员函数返回的是保护数据的指针或引用时，会破坏对数据的保护。**具有访问能力的指针或引用可以访问(并可能修改)被保护的数据，而不会被互斥锁限制。**互斥量保护的数据需要对接口的设计相当谨慎，要确保互斥量能锁住任何对保护数据的访问

我们要区别“将所有可访问的数据结构代码标记为互斥”与“对数据进行保护”。

```cpp
class some_data
{
int a;
std::string b;
public:
void do_something();
};

class data_wrapper
{
private:
some_data data;
std::mutex m;
public:
template<typename Function>
void process_data(Function func)
{
 std::lock_guard<std::mutex> l(m);
 func(data);    // 1 传递“保护”数据给用户函数
}
};

some_data* unprotected;

void malicious_function(some_data& protected_data)
{
unprotected=&protected_data;
}

data_wrapper x;
void foo()
{
x.process_data(malicious_function);    // 2 传递一个恶意函数
unprotected->do_something();    // 3 在无保护的情况下访问保护数据
}
```

例子中``process_data``看起来没有任何问题，``std::lock_guard``对数据做了很好的保护，但调用用户提供的函数func①，就意味着``foo``能够绕过保护机制将函数``malicious_function``传递进去②，在没有锁定互斥量的情况下调用``do_something()``。这段代码的问题在于根本没有保护，只是将所有可访问的数据结构代码标记为互斥。函数``foo()``中调用``unprotected->do_something()``的代码未能被标记为互斥。

我们建议：

> 1. 确保成员函数不会传出受保护数据的指针或引用
> 2. 检查成员函数是否通过指针或引用的方式来调用
> 3. **切勿将受保护数据的指针或引用传递到互斥锁作用域之外，无论是函数返回值，还是存储在外部可见内存，亦或是以参数的形式传递到用户提供的函数中去。**

#### 接口设计

```cpp
stack<int> s;
if (! s.empty()){    // 1
    //调用empty()①和调用top()②之间，可能有来自另一个线程的pop()调用并删除了最后一个元素
  int const value = s.top();    // 2
  s.pop();    // 3
  do_something(value);
}
```

对于共享的栈对象，这样的调用顺序就不再安全，因为在调用``empty()``①和调用``top()``②之间，可能有来自另一个线程的``pop()``调用并删除了最后一个元素。这是一个经典的条件竞争，使用互斥量对栈内部数据进行保护，但依旧不能阻止条件竞争的发生，这就是接口固有的问题。

以构建线程安全的栈为例，我们可以较为直观地学习一个需要判断状态`empty()`和读`pop()`写`push()`操作的类是如何实现线程安全的。

首先，在进行接口设计时，我们提供三个选项供实践时自由组合：

1.  **传入一个引用**

   将变量的引用作为参数，传入``pop()``函数中获取想要的“弹出值”

2. **无异常抛出的拷贝构造函数或移动构造函数**

3. **返回指向弹出值的指针**

   返回一个指向弹出元素的指针，而不是直接返回值，这里建议使用``std::shared_ptr``

```cpp
#include <exception>
#include <memory>
#include <mutex>
#include <stack>

struct empty_stack: std::exception
{
const char* what() const throw() {
 return "empty stack!";
};
};

template<typename T>
class threadsafe_stack
{
private:
std::stack<T> data;
mutable std::mutex m;

public:
threadsafe_stack()
 : data(std::stack<T>()){}

threadsafe_stack(const threadsafe_stack& other)
{
 std::lock_guard<std::mutex> lock(other.m);
 data = other.data; // 1 在构造函数体中的执行拷贝
}

threadsafe_stack& operator=(const threadsafe_stack&) = delete;

void push(T new_value)
{
 std::lock_guard<std::mutex> lock(m);
 data.push(new_value);
}

std::shared_ptr<T> pop()
{
 std::lock_guard<std::mutex> lock(m);
 ///
 if(data.empty()) throw empty_stack(); // 在调用pop前，检查栈是否为空

 std::shared_ptr<T> const res(std::make_shared<T>(data.top())); // 在修改堆栈前，分配出返回值
 ///
 data.pop();
 return res;
}

void pop(T& value)
{
 std::lock_guard<std::mutex> lock(m);
 if(data.empty()) throw empty_stack();

 value=data.top();
 data.pop();
}

bool empty() const
{
 std::lock_guard<std::mutex> lock(m);
 return data.empty();
}
};
```

接口会产生条件竞争，所以削减接口可以获得最大程度的安全,甚至限制对类的一些操作。简化接口更有利于数据控制，可以保证互斥量将一个操作完全锁住。

我们建议：

> 1. 尽可能地简化接口

### 死锁

一个给定操作需要两个或两个以上的互斥量时，另一个潜在的问题将出现：死锁(*deadlock*)。与条件竞争完全相反——不同的两个线程会互相等待，从而什么都没做。

避免死锁的一般建议，就是**让两个互斥量总以相同的顺序上锁**。

为此，C++提供了`std::lock`——可以一次性锁住多个(两个以上)的互斥量，并且没有副作用(死锁风险)。

```cpp
// 这里的std::lock()需要包含<mutex>头文件
class some_big_object;
void swap(some_big_object& lhs,some_big_object& rhs);
class X
{
private:
some_big_object some_detail;
std::mutex m;
public:
X(some_big_object const& sd):some_detail(sd){}

friend void swap(X& lhs, X& rhs)
{
 if(&lhs==&rhs)
   return;
 std::lock(lhs.m,rhs.m); // 1
 std::lock_guard<std::mutex> lock_a(lhs.m,std::adopt_lock); // 2
 std::lock_guard<std::mutex> lock_b(rhs.m,std::adopt_lock); // 3
 swap(lhs.some_detail,rhs.some_detail);
}
};
```

在此，我们对`std::lock_guard`有更深一层认识：虽然``std::lock_guard``的功能很简单，但是在构建``std::lock_guard<T>``对象时，我们可以通过不同的参数，对锁的状态进行确认。

| value                                               | **description**                                              |
| --------------------------------------------------- | ------------------------------------------------------------ |
| *(no tag)*                                          | Lock on construction by calling member lock                  |
| [try_to_lock](http://www.cplusplus.com/try_to_lock) | Attempt to lock on construction by calling member try_lock   |
| [defer_lock](http://www.cplusplus.com/defer_lock)   | Do not lock on construction (and assume it is not already locked by thread) |
| [adopt_lock](http://www.cplusplus.com/adopt_lock)   | Adopt current lock (assume it is already locked by thread).  |



对于死锁，还有一些进阶指导

- 避免嵌套锁
- 避免在持有锁时调用用户提供的代码
- 使用固定顺序获取锁
- 使用锁的层次结构

### ``unique_lock``

`std::unique_lock`与`std::lock_guard`没有很本质的区别。`unique_lock`的反义词是`shared_lock`，这样我们可以更好地理解`unique`：锁的所有权是被唯一占有的，是可移动，但不可拷贝的。

同时，``std::unique_lock``是更灵活的，从创建开始，到作用域结束，可以随意上锁解锁。

```cpp
class some_big_object;
void swap(some_big_object& lhs,some_big_object& rhs);
class X
{
private:
  some_big_object some_detail;
  std::mutex m;
public:
  X(some_big_object const& sd):some_detail(sd){}
  friend void swap(X& lhs, X& rhs)
  {
    if(&lhs==&rhs)
      return;
    std::unique_lock<std::mutex> lock_a(lhs.m,std::defer_lock); // 1
    std::unique_lock<std::mutex> lock_b(rhs.m,std::defer_lock); // 1 std::def_lock 留下未上锁的互斥量
    std::lock(lock_a,lock_b); // 2 互斥量在这里上锁
    swap(lhs.some_detail,rhs.some_detail);
  }
};
```

我们提出与**死锁**那节不同的代码进行对比

```cpp
 std::lock(lhs.m,rhs.m); // 1
 std::lock_guard<std::mutex> lock_a(lhs.m,std::adopt_lock); // 2
 std::lock_guard<std::mutex> lock_b(rhs.m,std::adopt_lock); // 3
 
 std::unique_lock<std::mutex> lock_a(lhs.m,std::defer_lock); // 1
 std::unique_lock<std::mutex> lock_b(rhs.m,std::defer_lock); // 1 std::def_lock留下未上锁的互斥量
 std::lock(lock_a,lock_b); // 2 互斥量在这里上锁
```

我们对两个类型的名字进行翻译：

- lock_guard：锁的看守
- unique_lock：唯一的锁

因此``lock_guard``不是锁，只是对已有的锁（创建``lock_guard``的参数）的操作进行管理：``lock_guard``创建时必须上锁，析构时必须解锁。而``lock_guard``构造函数的第二个参数，只是辅助``lock_guard``对锁的状态进行判断，防止一个锁连续上锁。

而``unique_lock``是一种锁，是用已有的锁（创建``lock_guard``的参数）创建的唯一锁，这个唯一锁拥有锁的全部内存所有权。显而易见的，如果锁没有上锁，那么`unique_lock`就要上锁；如果锁需要解锁，那么`unique_lock`就要解锁；如果`unique_lock`被析构，锁也被析构。``unique_lock``的第二个参数，也是对锁的状态进行判断。

`std::unique_lock`会占用比较多的空间，并且比``std::lock_guard``稍慢一些。保证灵活性要付出代价，这个代价就是允许`std::unique_lock`实例不带互斥量：信息已被存储，且已被更新。

我们建议：

> 1. 灵活有代价，在能使用`std::lock_guard`的地方尽可能使用

对于``std::unique_lock``的可移动性，有一下情形：

```cpp
class LockDemo
{
public:
	void BigFunc()
	{
		std::unique_lock<std::mutex> lock(m_mutex);
		// prepare data
		// process data
		// clean data
	}
private:
	std::mutex m_mutex;
};
```

根据*单一职责原则（Single Responsibility Principle）*，我们将其拆分：

```cpp
class LockDemo
{
public:
	std::unique_lock<std::mutex> GetLock()
	{
		std::unique_lock<std::mutex> lock(m_mutex);
		return lock;
	}
public:
	void BigFunc()
	{
		auto lock = GetLock();
		CleanData(ProcessData(PrepareData(std::move(lock))));//这里貌似有问题
	}
public:
	std::unique_lock<std::mutex> PrepareData(std::unique_lock<std::mutex> lock)
	{
		// ...
		return lock;
	}
	std::unique_lock<std::mutex> ProcessData(std::unique_lock<std::mutex> lock)
	{
		// ...
		return lock;
	}
	std::unique_lock<std::mutex> CleanData(std::unique_lock<std::mutex> lock)
	{
		// ...
		return lock;
	}
private:
	std::mutex m_mutex;
};
```

我们建议：

> 







### 锁的粒度

锁的粒度用来描述通过一个锁保护着的数据量大小。一个细粒度锁(*a fine-grained lock*)能够保护较小的数据量，一个粗粒度锁(*a coarse-grained lock*)能够保护较多的数据量。

```cpp
void get_and_process_data()
{
  std::unique_lock<std::mutex> my_lock(the_mutex);
  some_class data_to_process=get_next_data_chunk();
  my_lock.unlock();  // 1 不要让锁住的互斥量越过process()函数的调用
  result_type result=process(data_to_process);
  my_lock.lock(); // 2 为了写入数据，对互斥量再次上锁
  write_result(data_to_process,result);
}
```

不需要让锁住的互斥量越过对process()函数的调用，所以可以在函数调用①前对互斥量手动解锁，并且在之后对其再次上锁②。

我们建议：

> 1. 尽可能减小锁的粒度，在不需要时立刻`unlock`

### 保护共享数据的初始化

我们有的操作需要对源进行检查，在了解数据是否会被初始化后，再决定数据是否需要初始化，即延迟初始化（Lazy initialization）。

我们可以使用锁：

```cpp
std::shared_ptr<some_resource> resource_ptr;
std::mutex resource_mutex;

void foo()
{
  std::unique_lock<std::mutex> lk(resource_mutex);  // 所有线程在此序列化 
  if(!resource_ptr)
  {
    resource_ptr.reset(new some_resource);  // 只有初始化过程需要保护 
  }
  lk.unlock();
  resource_ptr->do_something();
}
```

不过，为了针对延迟初始化的线程安全，C++提供了`std::once_flag`和`std::call_once`。比起锁住互斥量，并显式的检查指针，每个线程只需要使用`std::call_once`，在`std::call_once`的结束时，就能安全的知道指针已经被其他的线程初始化了。**使用`std::call_once`比显式使用互斥量消耗的资源更少，特别是当初始化完成后。**如同大多数在标准库中的函数一样，或作为函数被调用，或作为参数被传递，`std::call_once`可以和任何函数或可调用对象一起使用。

```cpp
std::shared_ptr<some_resource> resource_ptr;
std::once_flag resource_flag;  // 1

void init_resource()
{
  resource_ptr.reset(new some_resource);
}

void foo()
{
  std::call_once(resource_flag,init_resource);  // 可以完整的进行一次初始化
  resource_ptr->do_something();
}
```

我们使用`std::call_once`作为类成员的延迟初始化(线程安全)

```cpp
class X
{
private:
  connection_info connection_details;
  connection_handle connection;
  std::once_flag connection_init_flag;

  void open_connection()
  {
    connection=connection_manager.open(connection_details);
  }
public:
  X(connection_info const& connection_details_):
      connection_details(connection_details_)
  {}
  void send_data(data_packet const& data)  // 1
  {
    std::call_once(connection_init_flag,&X::open_connection,this);  // 2
    connection.send_data(data);
  }
  data_packet receive_data()  // 3
  {
    std::call_once(connection_init_flag,&X::open_connection,this);  // 2
    return connection.receive_data();
  }
};
```

### 保护很少更新的数据结构

使用`std::mutex`来保护这样的数据结构，显的有些反应过度(因为在没有发生修改时，它将削减并发读取数据的可能性)。这里需要另一种不同的互斥量，这种互斥量常被称为“读者-写者锁”(*reader-writer mutex*)，因为其允许两种不同的使用方式：一个“作者”线程独占访问和共享访问，让多个“读者”线程并发访问。

为此我们使用`boost::shared_mutex`来做同步。

对于更新操作，可以使用`std::lock_guard<boost::shared_mutex>`和`std::unique_lock<boost::shared_mutex>`上锁。作为`std::mutex`的替代方案，与`std::mutex`所做的一样，这就能保证更新线程的独占访问。因为其他线程不需要去修改数据结构，所以其可以使用`boost::shared_lock<boost::shared_mutex>`获取访问权。这与使用`std::unique_lock`一样，除非多线程要在同时获取同一个`boost::shared_mutex`上有共享锁。唯一的限制：当任一线程拥有一个共享锁时，这个线程就会尝试获取一个独占锁，直到其他线程放弃他们的锁；同样的，当任一线程拥有一个独占锁时，其他线程就无法获得共享锁或独占锁，直到第一个线程放弃其拥有的锁。

```cpp
#include <map>
#include <string>
#include <mutex>
#include <boost/thread/shared_mutex.hpp>

class dns_entry;

class dns_cache
{
  std::map<std::string,dns_entry> entries;
  mutable boost::shared_mutex entry_mutex;
public:
  dns_entry find_entry(std::string const& domain) const
  {
    boost::shared_lock<boost::shared_mutex> lk(entry_mutex);  // 1
    std::map<std::string,dns_entry>::const_iterator const it=
       entries.find(domain);
    return (it==entries.end())?dns_entry():it->second;
  }
  void update_or_add_entry(std::string const& domain,
                           dns_entry const& dns_details)
  {
    std::lock_guard<boost::shared_mutex> lk(entry_mutex);  // 2
    entries[domain]=dns_details;
  }
};
```

我们建议：

> 1. 对很少更新的数据结构，在修改时使用`std::lock_guard<boost::shared_mutex>`和`std::unique_lock<boost::shared_mutex>`上锁；在读取时，使用`boost::shared_lock<boost::shared_mutex>`获取访问权。



## 3 同步并发操作

- 等待事件
- 带有期望的等待一次性事件
- 在限定时间内等待
- 使用同步操作简化代码

### 等待条件

当一个线程等待另一个线程完成任务时，它会有很多选择。第一，它可以持续的检查共享数据标志(用于做保护工作的互斥量)，直到另一线程完成工作时对这个标志进行重设。不过，就是一种浪费：**线程消耗宝贵的执行时间持续的检查对应标志**，并且当互斥量被等待线程上锁后，其他线程就没有办法获取锁，这样线程就会持续等待。因为以上方式对等待线程限制资源，并且在完成时阻碍对标识的设置。等待的线程会等待更长的时间，这些线程也在消耗着系统资源。

#### 条件

> 用于较高的频率，满足“条件”即触发

通过另一线程触发等待事件的机制是最基本的唤醒方式(例如：流水线上存在额外的任务时)，这种机制就称为“条件变量”(condition variable)。

C++标准库对条件变量有两套实现：``std::condition_variable``和``std::condition_variable_any``。这两个实现都包含在<condition_variable>头文件的声明中。两者都需要与一个互斥量一起才能工作(互斥量是为了同步)；前者仅限于与``std::mutex``一起工作，而后者可以和任何满足最低标准的互斥量一起工作，从而加上了*_any*的后缀。``std::condition_variable``一般作为首选的类型，当对灵活性有硬性要求时，我们才会去考虑``std::condition_variable_any``。

我们建议：

> 1. 以性能为主，优先考虑``std::condition_variable``

```cpp
std::mutex mut;
std::queue<data_chunk> data_queue;  // 1
std::condition_variable data_cond;

void data_preparation_thread()
{
  while(more_data_to_prepare())
  {
    data_chunk const data=prepare_data();
    std::lock_guard<std::mutex> lk(mut);
    data_queue.push(data);  // 2
    data_cond.notify_one();  // 3
  }
}

void data_processing_thread()
{
  while(true)
  {
    std::unique_lock<std::mutex> lk(mut);  // 4
    data_cond.wait(
         lk,[]{return !data_queue.empty();});  // 5
    data_chunk data=data_queue.front();
    data_queue.pop();
    lk.unlock();  // 6
    process(data);
    if(is_last_chunk(data))
      break;
  }
}
```

首先，你拥有一个用来在两个线程之间传递数据的队列①。当数据准备好时，使用std::lock_guard对队列上锁，将准备好的数据压入队列中②，之后线程会对队列中的数据上锁。然后调**用``std::condition_variable``的``notify_one()``成员函数，对等待的线程(如果有等待线程)进行通知**③。

线程之后会调用``std::condition_variable``的成员函数``wait()``，传递一个锁和一个lambda函数表达式(作为等待的条件⑤)。wait()会去检查这些条件(通过调用所提供的lambda函数)，当条件满足(lambda函数返回true)时返回。如果条件不满足(lambda函数返回false)，wait()函数将解锁互斥量，并且将这个线程(上段提到的处理数据的线程)置于阻塞或等待状态。当准备数据的线程调用notify_one()通知条件变量时，处理数据的线程从睡眠状态中苏醒，重新获取互斥锁，并且对条件再次检查，在条件满足的情况下，从wait()返回并继续持有锁。当条件不满足时，线程将对互斥量解锁，并且重新开始等待。**这就是为什么用std::unique_lock而不使用std::lock_guard——等待中的线程必须在等待期间解锁互斥量，并在这这之后对互斥量再次上锁，而std::lock_guard没有这么灵活**。如果互斥量在线程休眠期间保持锁住状态，准备数据的线程将无法锁住互斥量，也无法添加数据到队列中；同样的，等待线程也永远不会知道条件何时满足。（没看懂）

我们建议：

> 1. 条件变量使用于较高频率的检查
> 2. 尽量使用`notify_one()`，而不是 `notify_all()`
> 3. 使用`std::unique_lock`



### 等待期望

> 使用期望等待一次性事件

C++标准库模型将这种一次性事件称为“期望” (*future*)。**当一个线程需要等待一个特定的一次性事件时，在某种程度上来说它就需要知道这个事件在未来的表现形式。**之后，这个线程会周期性(较短的周期)的等待或检查，事件是否触发(检查信息板)；在检查期间也会执行其他任务(品尝昂贵的咖啡)。另外，在等待任务期间它可以先执行另外一些任务，直到对应的任务触发，而后等待期望的状态会变为“就绪”(*ready*)。一个“期望”可能是数据相关的，也可能不是。当事件发生时(并且期望状态为就绪)，这个“期望”就不能被重置。

> `std::future`[^std::future] 是异步返回对象

在C++标准库中，有两种“期望”，使用两种类型模板实现，声明在头文件中: 唯一期望(*unique futures*)(`std::future<>`)和共享期望(*shared futures*)(`std::shared_future<>`)。

我们为`std::future`提供三种使用组合：

- `std::async`
- `std::packaged_task`：
- `std::promise`

#### 线程返回值与期望

`std::thread`并不提供直接接收返回值的机制。这里就需要``std::async``函数模板。``std::future`` 用来存放``std::async``开辟线程的返回值。

`std::async`就是异步编程的高级封装，封装了``std::future`的操作，基本上可以代替`std::thread` 的所有事情。**`std::async`的操作，其实相当于封装了`std::promise`、`std::packaged_task`加上`std::thread`。**

`std::async`[^async的参数]

```cpp
#include <future>
#include <iostream>

int find_the_answer_to_ltuae();
void do_other_stuff();
int main()
{
  std::future<int> the_answer=std::async(find_the_answer_to_ltuae);
  do_other_stuff();
  std::cout<<"The answer is "<<the_answer.get()<<std::endl;
}
```

`std::async`有两个额外线程控制参数

- `std::launch::defered`：表明函数调用被延迟到wait()或get()函数调用时才执行
- `std::launch::async`：表明函数必须在其所在的独立线程上执行，

我们建议：

> 1. 多使用`std::async`，它比`std::thread`更简单安全

#### 任务与期待

`std::packaged_task<>`[^packaged_task的构造]对一个函数或可调用对象，绑定一个期望。当``std::packaged_task<> ``对象被调用，它就会调用相关函数或可调用对象，将期望状态置为就绪，返回值也会被存储为相关数据。

当一个粒度较大的操作可以被分解为独立的子任务时，其中每个子任务就可以包含在一个``std::packaged_task<>``实例中，之后这个实例将传递到任务调度器或线程池中。这就是对任务的细节进行抽象了；**调度器仅处理``std::packaged_task<>``实例，要比处理独立的函数高效的多。**

当线程间传递任务时，需要一个线程发出信息给正确的线程，让特定的线程工作。``std::packaged_task``提供了完成这种功能的一种方法。

```cpp
#include <deque>
#include <mutex>
#include <future>
#include <thread>
#include <utility>

std::mutex m;
std::deque<std::packaged_task<void()> > tasks;

bool gui_shutdown_message_received();
void get_and_process_gui_message();

void gui_thread()  // 1
{
  while(!gui_shutdown_message_received())  // 2
  {
    get_and_process_gui_message();  // 3
    std::packaged_task<void()> task;
    {
      std::lock_guard<std::mutex> lk(m);
      if(tasks.empty())  // 4
        continue;
      task=std::move(tasks.front());  // 5 提取出一个任务
      tasks.pop_front();
    }
    task();  // 6 执行任务
  }
}

std::thread gui_bg_thread(gui_thread);

template<typename Func>
std::future<void> post_task_for_gui_thread(Func f)
{
  std::packaged_task<void()> task(f);  // 7
  std::future<void> res=task.get_future();  // 8 获取“期望”对象
  std::lock_guard<std::mutex> lk(m);  // 9
  tasks.push_back(std::move(task));  // 10
  return res;
}
```

我们建议：

> 1. 用``std::packaged_task<>``来封装任务，并移动到线程中执行

#### 承诺与期望

`std::future` 是异步返回对象， `std::async`  ，`std::packaged_task`  ，和`std::promise` 都是异步提供程序。相比之下``std::promise``的功能更底层，**它适合用在设置`std::future`的代码无法被封装在一个简单的函数中传递给`std::async`的时候。**

``std::promise``的成员函数`get_future()`是对`std::future`进行承诺（绑定），即时刻根据线程结果`set_value()`(设置)`std::future`。

```cpp
#include <iostream>
#include <future>
#include <chrono>
#include <functional>
 
//声明一个可调对象T
using T = std::function<int(int)>;		//等同于typedef std::function<int(int)> T;
 
int Test_Fun(int iVal)
{
	std::cout << "Value is:" << iVal << std::endl;
	return iVal + 232;
}
 
void Thread_Fun1(std::promise<T> &p)
{
	//为了突出效果，可以使线程休眠5s
	std::this_thread::sleep_for(std::chrono::seconds(5));
 
	std::cout << "传入函数Test_Fun" << std::endl;
 
	//传入函数Test_Fun
	p.set_value(std::bind(&Test_Fun, std::placeholders::_1));
}
 
void Thread_Fun2(std::future<T> &f)
{
	//阻塞函数，直到收到相关联的std::promise对象传入的数据
	auto fun = f.get();		//iVal = 233
 
	int iVal = fun(1);
 
	std::cout << "收到函数并运行，结果：" << iVal << std::endl;
}
 
int main()
{
	//声明一个std::promise对象pr1，其保存的值类型为int
	std::promise<T> pr1;
	//声明一个std::future对象fu1，并通过std::promise的get_future()函数与pr1绑定
	std::future<T> fu1 = pr1.get_future();
 
	//创建一个线程t1，将函数Thread_Fun1及对象pr1放在线程里面执行
	std::thread t1(Thread_Fun1, std::ref(pr1));
	//创建一个线程t2，将函数Thread_Fun2及对象fu1放在线程里面执行
	std::thread t2(Thread_Fun2, std::ref(fu1));
 
	//阻塞至线程结束
	t1.join();
	t2.join();
 
	return 1;
}
```

如下例**单线程处理多接口**的实现，我们可以使用一对``std::promise<bool>/std::future<bool>``找出一块传出成功的数据块；与“期望”相关值只是一个简单的“成功/失败”标识。对于传入包，与“期望”相关的数据就是数据包的有效负载。

```cpp
#include <future>

void process_connections(connection_set& connections)
{
  while(!done(connections))  // 1
  {
    for(connection_iterator  // 2
            connection=connections.begin(),end=connections.end();
          connection!=end;
          ++connection)
    {
      if(connection->has_incoming_data())  // 3
      {
        data_packet data=connection->incoming();
        std::promise<payload_type>& p=
            connection->get_promise(data.id);  // 4
        p.set_value(data.payload);
      }
      if(connection->has_outgoing_data())  // 5
      {
        outgoing_packet data=
            connection->top_of_outgoing_queue();
        connection->send(data.payload);
        data.promise.set_value(true);  // 6 当发送完成，与传出数据相关的“承诺”将置为true，来表明传输成功
      }
    }
  }
}
```



#### 期望与异常

当你希望存入的是一个异常而非一个数值时，你就需要调用``set_exception()``成员函数，而非``set_value()``。这通常是用在一个catch块中，并作为算法的一部分，为了捕获异常，使用异常填充“承诺”：

```cpp
extern std::promise<double> some_promise;
try
{
  some_promise.set_value(calculate_value());
}
catch(...)
{
  some_promise.set_exception(std::current_exception());
}
```

当异常类型已知时，可以使用``std::copy_exception()``，它会直接存储一个新的异常而不抛出，**给编译器提供了极大的代码优化空间**：

```cpp
some_promise.set_exception(std::copy_exception(std::logic_error("foo ")));
```

另一种向“期望”中存储异常的方式是，在没有调用“承诺”上的任何设置函数前，或正在调用包装好的任务时，销毁与``std::promise``或``std::packaged_task``相关的“期望”对象。在这任何情况下，当“期望”的状态还不是“就绪”时，调用``std::promise``或``std::packaged_task``的析构函数，将会存储一个与``std::future_errc::broken_promise``错误状态相关的``std::future_error``异常。

我们建议：

> 1. 当异常类型已知时，使用``std::copy_exception()``
> 2. 注意`std::future`的析构是否在被设置之前

#### 小结

[异步计算的抽象层次](https://stackoverflow.com/questions/11004273/what-is-stdpromise)

##### 异步计算的层级关系

`std::future`应该被认为是普通返回类型的异步替代，而 `std::async`  ，`std::packaged_task`  ，和`std::promise` 都是这个返回对象的异步提供程序，它们间存在着层级关系。

~~`std::future`有两个面向不同方向的接口，`wait_for()`和`get()`。~~

```cpp
int foo(double, char, bool);//示例函数
```

层级由高到低：

1. `std::async`：异步编程的高级封装，**相当于封装了`std::promise`、`std::packaged_task`加上`std::thread`。**它可以直接返回`std::future`。`std::async`可以大大扁平化异步操作。

   ```cpp
   auto fut = std::async(foo, 1.5, 'x', false);  // is a std::future<int>
   auto res = fut.get();  // is an int
   ```

2. `std::thread`：拥有异步线程的内存空间，需要一个可以调用的函数作为参数被移动。

3. `std::packaged_task`：能够封装一个函数，并且根据函数返回值提供一个`std::future`，并且`std::packged_task`是 可调用的。我们可以把一个`std::paackaged_task`移动到`std::thread`中。

   ```cpp
   std::packaged_task<int(double, char, bool)> tsk(foo);
   auto fut = tsk.get_future();    // is a std::future<int>
   
   std::thread thr(std::move(tsk), 1.5, 'x', false);//move
   auto res = fut.get();  // as before
   ```

4. `std::promise`：是最基础的与`std::future`通信的模块。一般流程如下：

   - The calling thread makes a promise.
   - The calling thread obtains a future from the promise.
   - The promise, along with function arguments, are moved into a separate thread.
   - The new thread executes the function and fulfills the promise.
   - The original thread retrieves the result.

   ```cpp
   template <typename> class my_task;
   
   template <typename R, typename ...Args>
   class my_task<R(Args...)>
   {
       std::function<R(Args...)> fn;
       std::promise<R> pr;             // the promise of the result
   public:
       template <typename ...Ts>
       explicit my_task(Ts &&... ts) : fn(std::forward<Ts>(ts)...) { }
   
       template <typename ...Ts>
       void operator()(Ts &&... ts)
       {
           pr.set_value(fn(std::forward<Ts>(ts)...));  // fulfill the promise
       }
   
       std::future<R> get_future() { return pr.get_future(); }
   
       // disable copy, default move
   };
   ```

   

### 多个线程等待

**`std::future`是只移动的，所以其所有权可以在不同的实例中互相传递，但是只有一个实例可以获得特定的同步结果，因此在第一次调用``get()``后，就没有值可以再获取了。**

为此，我们使用``std::shared_future``。``std::shared_future``实例是可拷贝的，所以多个对象可以引用同一关联“期望”的结果。同时，我们需要锁来保护访问。

``std::shared_future``支持拷贝构造和移动构造，同时支持一次隐式转换。同时也可以使用`std::future`的`share()`成员函数来创建新的``std::shared_future``。

```cpp
std::promise<int> p;
std::future<int> f(p.get_future());
assert(f.valid());  // 1 "期望" f 是合法的
std::shared_future<int> sf(std::move(f));
assert(!f.valid());  // 2 "期望" f 现在是不合法的
assert(sf.valid());  // 3 sf 现在是合法的

std::promise<std::string> p;
std::shared_future<std::string> sf(p.get_future());  // 4 隐式转移所有权

std::promise< std::map< SomeIndexType, SomeDataType, SomeComparator,
     SomeAllocator>::iterator> p;
auto sf=p.get_future().share();//
```

我们建议：

> 

### 使用同步操作简化代码

#### FP

在同步工具的选择上，我们推荐`std::future`，**比起在多个线程间直接共享数据，每个任务拥有自己的数据会应该会更好，并且结果可以对其他线程进行广播，这就需要使用“期望”来完成了。**

在编程模式上，我们推荐使用“函数化编程”(*functional programming ( FP )*)，这种方式中的函数结果**只依赖于传入函数的参数，并不依赖外部状态。**

函数化编程模式并发化(FP-style concurrency)的四块拼图：

- lambda表达式

- `std::bind`

- `auto`

  ~可以自行推断类型的自动变量~

- `std::future`

  ~可以在线程间互相传递，并允许其中一个计算结果依赖于另外一个的结果，而非对共享数据的显式访问。~

下例为使用 FP模式进行快速排序

```cpp
template<typename T>
std::list<T> parallel_quick_sort(std::list<T> input)
{
  if(input.empty())
  {
    return input;
  }
  std::list<T> result;
  result.splice(result.begin(),input,input.begin());
  T const& pivot=*result.begin();

  auto divide_point=std::partition(input.begin(),input.end(),
                [&](T const& t){return t<pivot;});

  std::list<T> lower_part;
  lower_part.splice(lower_part.end(),input,input.begin(),
                divide_point);

  std::future<std::list<T> > new_lower(  // 1
                std::async(&parallel_quick_sort<T>,std::move(lower_part)));

  auto new_higher(
                parallel_quick_sort(std::move(input)));  // 2

  result.splice(result.end(),new_higher);  // 3
  result.splice(result.begin(),new_lower.get());  // 4
  return result;
}
```

为了避免大量的拷贝，使用了了std::partition()，std::move()

当前线程不对小于“中间”值部分的列表进行排序，使用``std::async()``①在另一线程对其进行排序。大于部分列表，如同之前一样，使用递归的方式进行排序②。通过**递归调用**parallel_quick_sort()，你就可以利用可用的硬件并发了。``std::async()``会启动一个新线程，这样当你递归三次时，就会有八个线程在运行了。

我们建议：

> 1. 可以尝试使用**递归操作**来快速产生大量线程

为了我们的线程池铺路，我们创建一个``spawn_task``类对``std::packaged_task``和`std::thread`进行简单包装。其本身不会提供太多好处，甚至实际上会造成大规模超额任务，但是它为线程池做了铺垫。

```cpp
template<typename F,typename A>
std::future<std::result_of<F(A&&)>::type>
   spawn_task(F&& f,A&& a)
{
  typedef std::result_of<F(A&&)>::type result_type;
  std::packaged_task<result_type(A&&)>
       task(std::move(f)));
  std::future<result_type> res(task.get_future());
  std::thread t(std::move(task),std::move(a));
  t.detach();
  return res;
}
```

我们建议：

> 1. 使用``std::async``更适合于当你知道你在干什么，并且要完全控制在线程池中构建或执行过任务的线程。
> 2. 线程池



#### CSP

因为避开了共享易变数据，函数化编程可算作是并发编程的范型；而CSP(*Communicating Sequential Processer*，通讯顺序进程)更进一步：没有共享数据，每个线程就可以进行独立思考，其行为纯粹基于其所接收到的信息。每个线程就都有一个状态机：当线程收到一条信息，它将会以某种方式更新其状态，并且可能向其他线程发出一条或多条信息，对于消息的处理依赖于线程的初始化状态。

下例为一台ATM机的状态机模型(简化)

```cpp
struct card_inserted
{
  std::string account;
};

class atm
{
  messaging::receiver incoming;
  messaging::sender bank;
  messaging::sender interface_hardware;
  void (atm::*state)();

  std::string account;
  std::string pin;

  void waiting_for_card()  // 1
  {
    interface_hardware.send(display_enter_card());  // 2
    incoming.wait().  // 3
      handle<card_inserted>(
      [&](card_inserted const& msg)  // 4
      {
       account=msg.account;
       pin="";
       interface_hardware.send(display_enter_pin());
       state=&atm::getting_pin;
      }
    );
  }
  void getting_pin();
public:
  void run()  // 5
  {
    state=&atm::waiting_for_card;  // 6
    try
    {
      for(;;)
      {
        (this->*state)();  // 7
      }
    }
    catch(messaging::close_queue const&)
    {
    }
  }
};
```

这种程序设计的方式被称为参与者模式([Actor model](http://zh.wikipedia.org/wiki/參與者模式))——在系统中有很多独立的(运行在一个独立的线程上)参与者，这些参与者会互相发送信息，去执行手头上的任务，并且它们不会共享状态，除非是通过信息直接传入的。









































[^async的参数]:`std::async`第一个参数是一个指向成员函数的指针，第二个参数提供有这个函数成员类的具体对象(不是直接的，就是通过指针，还可以包装在``std::ref``中)，剩余的参数可作为成员函数的参数传入。当参数为右值(*rvalues*)时，拷贝操作将使用移动的方式转移原始数据。这就允许使用“只移动”类型作为函数对象和参数。
[^packaged_task的构造]:当你构造出一个``std::packaged_task<>``实例时，你必须传入一个函数或可调用对象，这个函数或可调用的对象需要能接收指定的参数和返回可转换为指定返回类型的值。类型可以不完全匹配；你可以用一个int类型的参数和返回一个float类型的函数，来构建``std::packaged_task<double(double)>``的实例，因为在这里，**类型可以隐式转换**。
[^ std::future]:``std::future``有三种状态：deferred，ready，timeout



